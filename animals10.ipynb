{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eedbf5d3",
   "metadata": {},
   "source": [
    "## TODO\n",
    "rename file folders -- done \n",
    "check if differenet image count per class is problem -- done\n",
    "load the dataset -- done\n",
    "resize images via PIL Image load > convert to torch tensor or directly loading it via torchvision read image -- done\n",
    "pridat vahovanie pre triedy ktorych je menej poctov - teda lepsia stratova funkcia\n",
    "!!!  este pre vizualizaciu, confusion matrix - spravne je na diagonale  -- done \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169cf9ee",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c09e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "!{sys.executable} -m pip install torch torchvision matplotlib numpy \n",
    "from collections import Counter\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from random import choice\n",
    "from uuid import uuid4\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "#for faster training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import numpy as np\n",
    "#to load the dataset and to split the dataset\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "#for resizing of images\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "use_wandb = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply getting working directory and appending the archive directories to it\n",
    "base_dir = os.getcwd()\n",
    "dataset_dir = os.path.join(base_dir, \"archive/raw-img\")\n",
    "print(\"Dataset directory: \", dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of the image that will be after \"normalization\" (setting all the images to the same size)\n",
    "img_size = (224, 224)\n",
    "#transormation that will be used on every image https://docs.pytorch.org/vision/stable/transforms.html\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "analysis_transform = transforms.ToTensor()\n",
    "\n",
    "#loading the dataset with pytorch using ImageFolder https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.ImageFolder.html\n",
    "\n",
    "image_dataset = ImageFolder(\n",
    "    root = dataset_dir, \n",
    "    transform = image_transform \n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#if you see ragno, my translate_names.py is not working correctly xd (its spider)\n",
    "print(\"Dataset classes: \", image_dataset.classes)\n",
    "\n",
    "#splitting the dataset to 70% train, 10% validation and 20% train using random_split   https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.random_split\n",
    "\n",
    "dataset_size = len(image_dataset)\n",
    "print(\"Dataset Size:\\t\", dataset_size)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "validation_size   = int(0.1 * dataset_size)\n",
    "test_size  = dataset_size - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(image_dataset, [train_size, validation_size, test_size])\n",
    "print(\"Split sizes:\\nTrain:\\t\\t\", len(train_dataset), \"\\nValidation:\\t\", len(validation_dataset), \"\\nTest:\\t\\t\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3ef2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the image_dataset to dataLoader https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset,\n",
    "    shuffle = True, #this shuffles order of images each epoch, random_split randomizes it only once, maybe we dont need this\n",
    "    batch_size = 10  #1 is default, maybe experiment with this later\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    dataset = validation_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = 10  \n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset = test_dataset,\n",
    "    shuffle = True,\n",
    "    batch_size = 10  \n",
    ")\n",
    "\n",
    "#this is how we can iterate through the DataLoader\n",
    "for image_tensor, labels in train_loader:\n",
    "    #shape is [Batch_size, Channels, Height, Width]\n",
    "    print(image_tensor.shape)\n",
    "    #and label (correct class of the image) for each image in tensor (images)\n",
    "    print(labels) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate = {\n",
    "    \"cane\": \"dog\",\n",
    "    \"cavallo\": \"horse\",\n",
    "    \"elefante\": \"elephant\",\n",
    "    \"farfalla\": \"butterfly\",\n",
    "    \"gallina\": \"chicken\",\n",
    "    \"gatto\": \"cat\",\n",
    "    \"mucca\": \"cow\",\n",
    "    \"pecora\": \"sheep\",\n",
    "    \"scoiattolo\": \"squirrel\",\n",
    "    \"ragno\": \"spider\",\n",
    "}\n",
    "\n",
    "\n",
    "def rename_dirs(base_path: str):\n",
    "    for fn in os.listdir(base_path):\n",
    "        full = os.path.join(base_path, fn)\n",
    "\n",
    "        if not os.path.isdir(full):\n",
    "            continue\n",
    "        if fn not in translate:\n",
    "            continue\n",
    "\n",
    "        new_name = translate[fn]\n",
    "        print(f\"{fn}  ->  {new_name}\")\n",
    "        os.rename(full, os.path.join(base_path, new_name))\n",
    "\n",
    "\n",
    "def show_sample_images(dataset, num_images=10):\n",
    "    indices = random.sample(range(len(dataset)), num_images)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, label = dataset[idx] \n",
    "        if hasattr(img, \"permute\"):\n",
    "            img = img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        plt.subplot(2, (num_images + 1) // 2, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(dataset.classes[label])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1191ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data anylisis here >\n",
    "# loading dataset for data vizualization and analysis\n",
    "rename_dirs(dataset_dir)\n",
    "\n",
    "analysis_dataset = ImageFolder(root=dataset_dir, transform=analysis_transform)\n",
    "analysis_loader = DataLoader(dataset=analysis_dataset, shuffle=True, batch_size=10)\n",
    "\n",
    "print(\"Classes:\", analysis_dataset.classes)\n",
    "print(\"Classes Count:\", len(analysis_dataset.classes))\n",
    "\n",
    "counts = Counter([label for _, label in analysis_dataset.samples])\n",
    "\n",
    "print(\"\\nImages per class:\")\n",
    "for idx, cls_name in enumerate(analysis_dataset.classes):\n",
    "    print(f\"{cls_name}: {counts[idx]}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "values = [counts[i] for i in range(len(analysis_dataset.classes))]\n",
    "bars = plt.bar(analysis_dataset.classes, values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.title(\"Class Distribution\")\n",
    "\n",
    "for bar, value in zip(bars, values):\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        value,\n",
    "        str(int(value)),\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "show_sample_images(analysis_dataset, num_images=10)\n",
    "\n",
    "\n",
    "paths = [path for (path, _) in analysis_dataset.samples]\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "\n",
    "for p in paths:\n",
    "    with Image.open(p) as img:\n",
    "        w, h = img.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "\n",
    "widths = np.array(widths)\n",
    "heights = np.array(heights)\n",
    "\n",
    "print(\"Images size statistics:\")\n",
    "print(f\"Mean width:  {widths.mean():.2f}\")\n",
    "print(f\"Mean height: {heights.mean():.2f}\")\n",
    "print(f\"Min width:   {widths.min()},  Max width:   {widths.max()}\")\n",
    "print(f\"Min height:  {heights.min()}, Max height:  {heights.max()}\")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Width distribution histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "counts_w, bins_w, patches_w = plt.hist(\n",
    "    widths, bins=20, color=\"skyblue\", edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Image Width Distribution\")\n",
    "plt.xlabel(\"Width (px)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Titles above bars\n",
    "for count, patch in zip(counts_w, patches_w):\n",
    "    if count > 0:\n",
    "        plt.text(\n",
    "            patch.get_x() + patch.get_width() / 2,\n",
    "            count,\n",
    "            str(int(count)),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "# Height distribution histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "counts_h, bins_h, patches_h = plt.hist(\n",
    "    heights, bins=20, color=\"salmon\", edgecolor=\"black\"\n",
    ")\n",
    "plt.title(\"Image Height Distribution\")\n",
    "plt.xlabel(\"Height (px)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "for count, patch in zip(counts_h, patches_h):\n",
    "    if count > 0:\n",
    "        plt.text(\n",
    "            patch.get_x() + patch.get_width() / 2,\n",
    "            count,\n",
    "            str(int(count)),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            fontsize=8,\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sizes = []\n",
    "\n",
    "for path, _ in analysis_dataset.samples:\n",
    "    with Image.open(path) as img:\n",
    "        w, h = img.size\n",
    "        sizes.append((w, h, path))\n",
    "\n",
    "\n",
    "min_size = min(sizes, key=lambda x: x[0] * x[1])\n",
    "max_size = max(sizes, key=lambda x: x[0] * x[1])\n",
    "\n",
    "print(\"Smallest Image:\")\n",
    "print(f\"Size: {min_size[0]}x{min_size[1]}\")\n",
    "print(f\"Path: {min_size[2]}\")\n",
    "print()\n",
    "\n",
    "print(\"Largest Image:\")\n",
    "print(f\"Size: {max_size[0]}x{max_size[1]}\")\n",
    "print(f\"Path: {max_size[2]}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.hist2d(widths, heights, bins=30, cmap=\"inferno\")\n",
    "plt.colorbar(label=\"Number of images\")\n",
    "\n",
    "plt.title(\"2D Distribution of Image Sizes (Width vs Height)\")\n",
    "plt.xlabel(\"Width (px)\")\n",
    "plt.ylabel(\"Height (px)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb85b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train, val, test folders\n",
    "random.seed(42)\n",
    "\n",
    "raw_dir = dataset_dir\n",
    "base_dir = os.path.dirname(raw_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "for d in [train_dir, val_dir, test_dir]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.10\n",
    "test_ratio = 0.20\n",
    "\n",
    "valid_ext = (\".jpg\", \".jpeg\", \".png\", \".bmp\")\n",
    "\n",
    "for cls_name in os.listdir(raw_dir):\n",
    "    cls_path = os.path.join(raw_dir, cls_name)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "\n",
    "    files = [f for f in os.listdir(cls_path) if f.lower().endswith(valid_ext)]\n",
    "\n",
    "    if not files:\n",
    "        continue\n",
    "\n",
    "    random.shuffle(files)\n",
    "\n",
    "    n = len(files)\n",
    "    n_train = int(n * train_ratio)\n",
    "    n_val = int(n * val_ratio)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_files = files[:n_train]\n",
    "    val_files = files[n_train : n_train + n_val]\n",
    "    test_files = files[n_train + n_val :]\n",
    "\n",
    "    train_cls_dir = os.path.join(train_dir, cls_name)\n",
    "    val_cls_dir = os.path.join(val_dir, cls_name)\n",
    "    test_cls_dir = os.path.join(test_dir, cls_name)\n",
    "\n",
    "    os.makedirs(train_cls_dir, exist_ok=True)\n",
    "    os.makedirs(val_cls_dir, exist_ok=True)\n",
    "    os.makedirs(test_cls_dir, exist_ok=True)\n",
    "\n",
    "    for fname in train_files:\n",
    "        shutil.copy2(os.path.join(cls_path, fname), os.path.join(train_cls_dir, fname))\n",
    "\n",
    "    for fname in val_files:\n",
    "        shutil.copy2(os.path.join(cls_path, fname), os.path.join(val_cls_dir, fname))\n",
    "\n",
    "    for fname in test_files:\n",
    "        shutil.copy2(os.path.join(cls_path, fname), os.path.join(test_cls_dir, fname))\n",
    "\n",
    "    print(f\"{cls_name}: {n_train} train | {n_val} val | {n_test} test\")\n",
    "\n",
    "print(\"\\nNew split complete:\")\n",
    "print(\"Train:\", train_dir)\n",
    "print(\"Val:  \", val_dir)\n",
    "print(\"Test: \", test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd3ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = Path(train_dir)\n",
    "classes = [d for d in train_root.iterdir() if d.is_dir()]\n",
    "if not classes:\n",
    "    raise RuntimeError(f\"No class folders found in {train_root}\")\n",
    "\n",
    "target = max(len(list(d.glob('*'))) for d in classes)\n",
    "print(f\"Target per class: {target}\")\n",
    "\n",
    "aug_fns = [\n",
    "    lambda img: img.transpose(Image.FLIP_LEFT_RIGHT),\n",
    "    lambda img: img.rotate(10, expand=True),\n",
    "    lambda img: img.rotate(-10, expand=True),\n",
    "    lambda img: img.transpose(Image.FLIP_LEFT_RIGHT).rotate(10, expand=True),\n",
    "    lambda img: img.transpose(Image.FLIP_LEFT_RIGHT).rotate(-10, expand=True),\n",
    "]\n",
    "\n",
    "for cls in sorted(classes, key=lambda p: p.name):\n",
    "    files = [f for f in cls.iterdir() if f.is_file()]\n",
    "    need = target - len(files)\n",
    "    print(f\"{cls.name}: have {len(files)}, need {need}\")\n",
    "    if need <= 0:\n",
    "        continue\n",
    "    for _ in range(need):\n",
    "        src = choice(files)\n",
    "        with Image.open(src) as img:\n",
    "            img = img.convert('RGB')\n",
    "            aug_img = choice(aug_fns)(img)\n",
    "        out_path = cls / f\"{src.stem}_aug_{uuid4().hex[:6]}{src.suffix}\"\n",
    "        aug_img.save(out_path, quality=95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts = {p.name: len(list(p.glob('*'))) for p in Path(train_dir).iterdir() if p.is_dir()}\n",
    "print(train_counts)\n",
    "print('Total:', sum(train_counts.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "518e0077",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m pixel_sq_sum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m total_pixels \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_images, _ \u001b[38;5;129;01min\u001b[39;00m train_raw_loader:\n\u001b[1;32m     18\u001b[0m     batch_images \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m     batch_size, channels, height, width \u001b[38;5;241m=\u001b[39m batch_images\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1482\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data, worker_id)\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1482\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1444\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1443\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1444\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1445\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1275\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1278\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:262\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:429\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 429\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    933\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = os.path.join(base_dir, \"train\")\n",
    "val_path = os.path.join(base_dir, \"val\")\n",
    "test_path = os.path.join(base_dir, \"test\")\n",
    "\n",
    "preprocess_basic = transforms.Compose(\n",
    "    [transforms.Resize((224, 224)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "train_raw = ImageFolder(train_path, transform=preprocess_basic)\n",
    "\n",
    "train_raw_loader = DataLoader(dataset=train_raw, batch_size=512, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "pixel_sum = torch.zeros(3).to(device)\n",
    "pixel_sq_sum = torch.zeros(3).to(device)\n",
    "total_pixels = 0\n",
    "\n",
    "for batch_images, _ in train_raw_loader:\n",
    "    batch_images = batch_images.to(device)\n",
    "    batch_size, channels, height, width = batch_images.shape\n",
    "    num_pixels = batch_size * height * width\n",
    "    total_pixels += num_pixels\n",
    "\n",
    "    pixel_sum += batch_images.sum(dim=[0, 2, 3])\n",
    "    pixel_sq_sum += (batch_images**2).sum(dim=[0, 2, 3])\n",
    "\n",
    "mean = pixel_sum / total_pixels\n",
    "std = torch.sqrt(pixel_sq_sum / total_pixels - mean**2)\n",
    "\n",
    "print(\"Train Normalization Stats:\")\n",
    "print(\"Mean:\", mean.tolist())\n",
    "print(\"Std: \", std.tolist())\n",
    "\n",
    "img_size = (112,122)\n",
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_set = ImageFolder(train_path, transform=train_transforms)\n",
    "val_set = ImageFolder(val_path, transform=val_transforms)\n",
    "test_set = ImageFolder(test_path, transform=test_transforms)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)\n",
    "\n",
    "print(\"\\nData Preparation finished!\")\n",
    "print(\"Train samples:\", len(train_set))\n",
    "print(\"Val samples:  \", len(val_set))\n",
    "print(\"Test samples: \", len(test_set))\n",
    "\n",
    "#adding subsets for testing, later delete them\n",
    "# train_subset = Subset(train_set, range(500))\n",
    "# val_subset = Subset(val_set, range(500))\n",
    "# test_subset = Subset(val_set, range(500))\n",
    "\n",
    "# train_sub_loader = DataLoader(train_subset, batch_size=16, shuffle=True)\n",
    "# val_sub_loader = DataLoader(val_subset, batch_size=16, shuffle=False)\n",
    "# test_sub_loader = DataLoader(test_subset, batch_size=16, shuffle=False)\n",
    "# print(len(train_set), len(train_sub_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6476f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional Model   \n",
    "#Using pytorch convolutional layers https://docs.pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "#MaxPooling is used in the presentation in the VGG Net, lets use it as well https://docs.pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "\n",
    "#since we have multiclass classification, we will use softmax at the output  https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html \n",
    "class AnimalMLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_layers, kernel_size, stride, device):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        self.prev = 3\n",
    "        self.padding = 1\n",
    "        self.device = device\n",
    "        self.FCL_layers = torch.nn.ModuleList()\n",
    "        self.FCL_out_channels = hidden_layers[2]\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "        \n",
    "        \n",
    "        for out_channels in hidden_layers[0]:\n",
    "            #adding flat padding = 1\n",
    "            self.layers.append(torch.nn.Conv2d(self.prev, out_channels, self.kernel_size, self.stride, self.padding))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Conv2d(out_channels, out_channels, self.kernel_size, self.stride, self.padding))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.MaxPool2d(2))\n",
    "            \n",
    "            self.prev = out_channels\n",
    "            \n",
    "        for out_channels in hidden_layers[1]:\n",
    "            self.layers.append(torch.nn.Conv2d(self.prev, out_channels, self.kernel_size, self.stride, self.padding))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Conv2d(out_channels, out_channels, self.kernel_size, self.stride, self.padding))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.Conv2d(out_channels, out_channels, self.kernel_size, self.stride, self.padding))\n",
    "            self.layers.append(torch.nn.ReLU())\n",
    "            self.layers.append(torch.nn.MaxPool2d(2))\n",
    "            #using this to set all layers to gpu\n",
    "            self.prev = out_channels\n",
    "            \n",
    "        self.to(self.device)\n",
    "\n",
    "    def prepare_FCL_layers(self, input_size):\n",
    "        if len(self.FCL_layers) > 0:\n",
    "            return\n",
    "        #added 3 Fully Connected Layers and at the end add Softmax\n",
    "        self.FCL_layers.append(torch.nn.Flatten())\n",
    "        self.FCL_layers.append(torch.nn.Linear(input_size, self.FCL_out_channels[0]))\n",
    "        self.FCL_layers.append(torch.nn.ReLU())\n",
    "        self.FCL_layers.append(torch.nn.Linear(self.FCL_out_channels[0], self.FCL_out_channels[1]))\n",
    "        self.FCL_layers.append(torch.nn.ReLU())\n",
    "        self.FCL_layers.append(torch.nn.Linear(self.FCL_out_channels[1], self.FCL_out_channels[2]))\n",
    "        # self.FCL_layers.append(torch.nn.Softmax(dim=1))  #since we will use CrossEntropyLoss, which includes softmax, we dont need to add it here\n",
    "        #using this to set all layers to gpu\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        #going through convolutional layers\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        #since output of our convolution is: torch.Size([10, 512, 7, 7]) \n",
    "        #input for our FCL will be 512*7*7 = 25088\n",
    "        ## Preparing the FCL layers with the correct input size\n",
    "        self.prepare_FCL_layers(input.shape[1]*input.shape[2]*input.shape[3])\n",
    "        #going through fully connected layers\n",
    "        for layer in self.FCL_layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "def train_model(model, data_loader, optimizer, loss_function):\n",
    "    # print(\"train loss: \")\n",
    "    model.train()\n",
    "    for image_tensor, labels in data_loader:\n",
    "        #shape is [Batch_size, Channels, Height, Width]\n",
    "        #send data to gpu\n",
    "        image_tensor = image_tensor.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #reset gradients, do forward pass, compute loss, do backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(image_tensor)\n",
    "        loss = loss_function(output, labels)\n",
    "        # print(loss.item(), end=\" \")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print(\"FCL output:\", output.shape, model.softmax_for_print(output))\n",
    "\n",
    "        # break\n",
    "\n",
    "#since we dont want our model to be trained on this part of dataset, we dont calculate gradients, nor use optimizer, just use loss function to determine how good the model is on unseen data\n",
    "def validate_model(model, data_loader, loss_function):\n",
    "    model.eval()   #to set the model to evaluation mode\n",
    "\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    all_losses, all_probs, all_labels = [], [], []\n",
    "    \n",
    "    #torch.no_grad() to disable gradient computing\n",
    "    with torch.no_grad():\n",
    "        for image_tensor, labels in data_loader:\n",
    "            image_tensor = image_tensor.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            output = model.forward(image_tensor)\n",
    "            loss = loss_function(output, labels)\n",
    "            # print(\"\\nvalidation loss: \", loss.item())\n",
    "            # total_loss += loss.item() #accumulating loss for each \n",
    "            \n",
    "            predicted_classes = output.argmax(dim=1) #pick only the predicted classes of batch size\n",
    "            correct_predictions += (predicted_classes == labels).sum().item()  # this compares the predicted and real classes, counts how many are correct and itemizes it to a number\n",
    "            # print(\"correct prediction:\", correct_predictions)\n",
    "\n",
    "            \n",
    "            all_losses.append(loss.item())\n",
    "            all_labels.append(labels)\n",
    "            all_probs.append(model.softmax(output))\n",
    "        \n",
    "        all_labels = torch.cat(all_labels).cpu().numpy()   #pass labels to cpu\n",
    "        all_probs = torch.cat(all_probs).cpu().numpy()\n",
    "        all_predicted_classes = all_probs.argmax(axis=1)\n",
    "        \n",
    "        f1 = f1_score(all_labels, all_predicted_classes, average='macro')\n",
    "        accuracy = (all_predicted_classes == all_labels).mean()\n",
    "        average_loss = sum(all_losses) / len(all_losses)\n",
    "\n",
    "        \n",
    "        print(\"average loss: \", average_loss, \"acurracy: \", accuracy, \"F1 score:\", f1)\n",
    "    validation_output = {\n",
    "        \"av_loss\": average_loss, \n",
    "        \"acc\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"labels\": all_labels,\n",
    "        \"classes\": all_predicted_classes\n",
    "    }\n",
    "    return validation_output\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240dc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_number_of_epochs = 30\n",
    "# config = [\n",
    "#     {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "#      \"learning_rate\" : 0.0001\n",
    "#      },\n",
    "#     {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "#      \"learning_rate\" : 0.00001\n",
    "#      },\n",
    "#     {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "#      \"learning_rate\" : 0.001\n",
    "#      },\n",
    "# ]\n",
    "hidden_layers = [[64, 128], [256, 256, 256], [1024, 1024, len(test_set.classes)]] #this is kinda static, if this changes, output of convolution changes, so the expected input of FCL will change \n",
    "learning_rate = 0.0001\n",
    "best_model_f1 = -torch.inf\n",
    "best_model_path = \"best_model.pt\"\n",
    "#defining our model\n",
    "model = AnimalMLP(hidden_layers, 3, 1, device)\n",
    "# for currenct_config in configs:\n",
    "if use_wandb:\n",
    "    wandb_run = wandb.init(project=\"animal10\")  #, config=current_config\n",
    "    wandb_run.watch(model, log=\"all\")\n",
    "#defining loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "#Training\n",
    "for epoch in range(min_number_of_epochs):\n",
    "    print(\"\\nEpoch: \", epoch+1)\n",
    "    #we want, every epoch to train our model on train dataset and right after check validation dataset\n",
    "    train_model(model, train_loader, optimizer, loss_function)\n",
    "    validation_output = validate_model(model, val_loader, loss_function)\n",
    "    if use_wandb:\n",
    "        wandb_run.log(validation_output)\n",
    "    #saving best model   https://docs.pytorch.org/docs/stable/notes/serialization.html#recommend-saving-models\n",
    "    if validation_output[\"f1\"] > best_model_f1:\n",
    "        best_model_f1 = validation_output[\"f1\"]\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "if use_wandb:\n",
    "    wandb_run.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1d757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments part\n",
    "\n",
    "config = [\n",
    "    {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "     \"learning_rate\" : 0.0001\n",
    "     },\n",
    "    {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "     \"learning_rate\" : 0.00001\n",
    "     },\n",
    "    {\"hidden_layers\" : [[64, 128], [256, 512, 512], [4096, 4096, len(test_set.classes)]],\n",
    "     \"learning_rate\" : 0.001\n",
    "     },\n",
    "]\n",
    "\n",
    "best_model_per_config = {}\n",
    "\n",
    "for current_config in config:\n",
    "    hidden_layers = current_config[\"hidden_layers\"]\n",
    "    learning_rate = current_config[\"learning_rate\"]\n",
    "    print(\"Starting new experiment with config:\", current_config)\n",
    "    model = AnimalMLP(hidden_layers, 3, 1, device)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_model_f1 = -torch.inf\n",
    "    best_model_path = f\"best_model_{hashlib.md5(str(current_config).encode()).hexdigest()}.pt\"\n",
    "    last_val = None\n",
    "    #Training\n",
    "    for epoch in range(min_number_of_epochs):\n",
    "        print(\"Epoch: \", epoch+1)\n",
    "        train_model(model, train_loader, optimizer, loss_function)\n",
    "        validation_output = validate_model(model, val_loader, loss_function)\n",
    "        last_val = validation_output\n",
    "        if validation_output[\"f1\"] > best_model_f1:\n",
    "            best_model_f1 = validation_output[\"f1\"]\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "    best_model_per_config[str(current_config)] = {\n",
    "        \"path\": best_model_path,\n",
    "        \"best_f1\": best_model_f1,\n",
    "        \"last_val_loss\": None if last_val is None else last_val[\"av_loss\"],\n",
    "        \"last_val_acc\": None if last_val is None else last_val[\"acc\"],\n",
    "    }\n",
    "\n",
    "print(\"Summary of configs:\")\n",
    "for cfg, info in best_model_per_config.items():\n",
    "    print(cfg, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b24c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the best model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "\n",
    "#Testing\n",
    "testing_output = validate_model(model, test_loader, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d2e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#log confusion matrix of best model https://wandb.ai/wandb/plots/reports/Confusion-Matrix-Usage-and-Examples--VmlldzozMDg1NTM\n",
    "\n",
    "if use_wandb: \n",
    "    wandb_run = wandb.init(project=\"animal10\", reinit=True, name=f\"best_model_{random.randint(1,100)}\")\n",
    "    wandb_run.log({\"conf_mat\": wandb.plot.confusion_matrix(y_true=testing_output[\"labels\"], preds=testing_output[\"classes\"], class_names=test_set.classes)})\n",
    "    wandb_run.finish()\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(10,8))\n",
    "confusion_mat = confusion_matrix(testing_output[\"labels\"], testing_output[\"classes\"], labels=list(range(len(test_set.classes)))) \n",
    "sns.heatmap(confusion_mat, annot=True, xticklabels=test_set.classes, yticklabels=test_set.classes, fmt=\"d\", linewidths=0.5)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
